* 2010-10-29
Several days after starting on this challenge I've actually read the documentation properly rather than just skimming it.
One of the good ideas that the instructions.txt suggests is that one should keep a log of the lessons learned during the process of attempting this challenge.

So far we've learned that I don't read documentation as thoroughly as I should.
The book Language Implementation Patterns has been invaluable during this exercise. Despite Terence Parr's constant suggestions that parser writers should be using Antlr I'm sticking to my approach of doing it all by hand in order to get a better understanding. This may turn out to be a mistake as the complexity of the parser rises.

I'm also trying to see how long I can tolerate writing Java code in Textmate before I switch back to Intellij. So far it's not been as horrible as I remember. I'm having to use things like "javap" to look up documentation since I'm mostly working on this when I'm offline. It's nice to remember that it's possible to code without fancy IDEs and being able to instantly look up things on the internet.

My initial LL(1) parser has become an LL(2) then LL(k), where k currently equals 3, parser in order to properly handle multi-line paragraphs. 

I now have a nice process emerging where I have a failing acceptance test which drives the creation of an appropriate set of functional or  unit tests which drive the implementation of the code that will eventually make the acceptance test pass.

Acceptance test 6 involves parsing headers. I've mostly been aiming to keep the lexer simple and have the parser do all the clever manipulation of the stream of tokens. However the idea of generating the 100 sytactically meaningless asterisk tokens required by acceptance test 8 offends me on some aesthetic level.

The options are:
- Use the current lexer. Emit N asterisk tokens and have the parser count the number of asterisks to work out what kind of header node to use.
- Lexer with arbitrary lookahead. It would keep processing the asterisks until it had worked out what level of header it was processing. Then it would emit a special header token.
- Lexer consumes the entire line and emits a normal token with type header. The parser then consumes the asterisks and uses the count to create a HeaderNode with a level attribute and the text after the asterisks. This means I have to switch from a homogenous AST to a heterogenous AST.
- Lexer consumes the entire line and emits a normal token with type header. The parser then consumes the asterisks and uses the count to emit an AST node with a type field with value "Hn" where n is the number of asterisks. This allows me to keep using a homegenous AST but weakens type safety since sometimes that field won't have useful data.

* 2011-12-07
Looking at the options above it seems I went with allowing the addition of optional tags (inspired by the s-expressions used to describe the AST in the spec) on a homogeneous AST. Although since tags are optional there is a claim to be made that this is an heterogeneous AST.

* 2011-12-11
It took some time on the Eurostar from Paris but I worked out how to lex blockquotes. The breakthrough was realising that handlers in the lexer don't always have to return the matching token. The handler for whitespace can consume all the insignificant whitespace then recursively delegate to another handler then return the token from that handler.

I have too many calls to lookaheadToken(1). These should all be replaced by something like getNextToken() but then you have the calls to lookaheadToken(2) and lookaheadToken(3). Ugly but consistent probably wins for now.

I'm also aware that the Parser class has very little coverage by unit tests. That's currently because I think there's more value in testing the classes it uses rather than spending a lot of effort in mocking them up in order to unit test the parser. However I suspect this will bite me in the future.

A few hours later...it did. Strangely all my functional tests had an extra root node in the creation of the expected AST. This bug in the tests was hidden by a bug in the AST. The AST bug slipped by because of a lack of coverage in my AST tests.